-- Enable required extensions
create extension if not exists pg_trgm;

-- Books - canonical record
-- This is our single source of truth for book data, merged from all providers
create table if not exists books (
  id uuid primary key, -- UUIDv7 generated by our application (NOT provider IDs)
  title text not null, -- JSON: volumeInfo.title
  subtitle text, -- JSON: volumeInfo.subtitle
  description text, -- JSON: volumeInfo.description (HTML from providers)
  isbn10 text, -- Canonical ISBN-10 (best available from all sources)
  isbn13 text, -- Canonical ISBN-13 (best available from all sources)
  published_date date, -- JSON: volumeInfo.publishedDate (parsed from various formats)
  language text, -- JSON: volumeInfo.language (ISO 639-1 code like 'en')
  publisher text, -- JSON: volumeInfo.publisher
  page_count integer, -- JSON: volumeInfo.pageCount or volumeInfo.printedPageCount
  -- Removed edition_number and edition_group_key (replaced with work_clusters system)
  -- Canonical cover stored in book_image_links; books table no longer carries cover path
  slug text unique, -- SEO-friendly URL slug (e.g., 'harry-potter-philosophers-stone-j-k-rowling')
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  -- Generated search vector for full-text search
  search_vector tsvector generated always as (
    setweight(to_tsvector('english', coalesce(title, '')), 'A') ||
    setweight(to_tsvector('english', coalesce(subtitle, '')), 'B') ||
    setweight(to_tsvector('english', coalesce(publisher, '')), 'C') ||
    setweight(to_tsvector('english', coalesce(description, '')), 'D')
  ) stored
);

create index if not exists idx_books_isbn13 on books(isbn13) where isbn13 is not null;
create index if not exists idx_books_isbn10 on books(isbn10) where isbn10 is not null;
create unique index if not exists uq_books_isbn13 on books(isbn13) where isbn13 is not null;
create unique index if not exists uq_books_isbn10 on books(isbn10) where isbn10 is not null;
-- Removed idx_books_edition_group_key index (using work_clusters instead)
create index if not exists idx_books_slug on books(slug); -- Always index slug for URL lookups
create index if not exists idx_books_created_at on books(created_at desc);
create index if not exists idx_books_updated_at on books(updated_at desc);
create index if not exists idx_books_search_vector on books using gin (search_vector);

-- Table and column comments for books
comment on table books is 'Canonical book records - single source of truth merged from all providers';
comment on column books.id is 'UUIDv7 generated by our application (NOT provider IDs)';
comment on column books.title is 'Book title from volumeInfo.title';
comment on column books.subtitle is 'Book subtitle from volumeInfo.subtitle';
comment on column books.description is 'Book description from volumeInfo.description (HTML from providers)';
comment on column books.isbn10 is 'Canonical ISBN-10 (best available from all sources)';
comment on column books.isbn13 is 'Canonical ISBN-13 (best available from all sources)';
comment on column books.published_date is 'Publication date from volumeInfo.publishedDate';
comment on column books.language is 'Language code (ISO 639-1) from volumeInfo.language';
comment on column books.publisher is 'Publisher name from volumeInfo.publisher';
comment on column books.page_count is 'Page count from volumeInfo.pageCount or printedPageCount';
-- Removed edition_number and edition_group_key column comments (using work_clusters instead)
comment on column books.slug is 'SEO-friendly URL slug generated once at creation, remains stable even if title/authors change';

-- External IDs mapping for many-to-one association to canonical books
-- This includes Google Books ID, OpenLibrary ID, etc. with provider-specific metadata
-- IMPORTANT: One book can have multiple external IDs (Google, Amazon, OpenLibrary, etc.)
create table if not exists book_external_ids (
  id text primary key, -- NanoID (10 chars via IdGenerator.generate())
  book_id uuid not null references books(id) on delete cascade,
  source text not null, -- 'GOOGLE_BOOKS', 'OPEN_LIBRARY', 'AMAZON', etc.
  external_id text not null, -- Provider's ID (JSON: id for Google Books)
  -- Industry identifiers from this provider
  provider_isbn10 text, -- JSON: volumeInfo.industryIdentifiers[type=ISBN_10].identifier
  provider_isbn13 text, -- JSON: volumeInfo.industryIdentifiers[type=ISBN_13].identifier
  provider_oclc text, -- OCLC number if provided
  provider_lccn text, -- Library of Congress Control Number if provided
  provider_asin text, -- Amazon Standard Identification Number
  -- Provider-specific metadata
  info_link text, -- JSON: volumeInfo.infoLink (Google Books info page)
  preview_link text, -- JSON: volumeInfo.previewLink
  web_reader_link text, -- JSON: accessInfo.webReaderLink
  purchase_link text, -- Link to purchase page if available
  canonical_volume_link text, -- JSON: volumeInfo.canonicalVolumeLink
  average_rating numeric, -- JSON: volumeInfo.averageRating (provider's rating)
  ratings_count integer, -- JSON: volumeInfo.ratingsCount
  review_count integer, -- Number of reviews on this platform
  -- Provider-specific availability
  is_ebook boolean, -- JSON: saleInfo.isEbook
  pdf_available boolean, -- JSON: accessInfo.pdf.isAvailable
  epub_available boolean, -- JSON: accessInfo.epub.isAvailable
  embeddable boolean, -- JSON: accessInfo.embeddable
  public_domain boolean, -- JSON: accessInfo.publicDomain
  viewability text, -- JSON: accessInfo.viewability ('FULL', 'PARTIAL', 'NO_PAGES', 'ALL_PAGES')
  text_readable boolean, -- JSON: volumeInfo.readingModes.text
  image_readable boolean, -- JSON: volumeInfo.readingModes.image
  -- Content metadata
  print_type text, -- JSON: volumeInfo.printType ('BOOK', 'MAGAZINE')
  maturity_rating text, -- JSON: volumeInfo.maturityRating ('NOT_MATURE', 'MATURE')
  content_version text, -- JSON: volumeInfo.contentVersion
  text_to_speech_permission text, -- JSON: accessInfo.textToSpeechPermission
  -- Sale info
  saleability text, -- JSON: saleInfo.saleability ('FOR_SALE', 'NOT_FOR_SALE', 'FREE')
  country_code text, -- JSON: saleInfo.country or accessInfo.country
  is_ebook_for_sale boolean, -- Derived from saleInfo
  -- Pricing info if available
  list_price numeric,
  retail_price numeric,
  currency_code text,
  -- Work identifiers (for clustering editions)
  oclc_work_id text, -- OCLC Work ID from WorldCat
  openlibrary_work_id text, -- OpenLibrary work identifier (e.g., /works/OL45804W)
  goodreads_work_id text, -- Goodreads work ID for grouping editions
  google_canonical_id text, -- Google Books ID extracted from canonicalVolumeLink

  -- Metadata
  last_updated timestamptz,
  created_at timestamptz not null default now(),
  unique(source, external_id)
);

create index if not exists idx_book_external_ids_book_id on book_external_ids(book_id);
create index if not exists idx_book_external_ids_external_id on book_external_ids(external_id);
create index if not exists idx_book_external_ids_source_id on book_external_ids(source, external_id);
create unique index if not exists uq_book_external_ids_provider_isbn13
  on book_external_ids(source, provider_isbn13) where provider_isbn13 is not null;
create unique index if not exists uq_book_external_ids_provider_isbn10
  on book_external_ids(source, provider_isbn10) where provider_isbn10 is not null;
create index if not exists idx_book_external_ids_isbn13 on book_external_ids(provider_isbn13) where provider_isbn13 is not null;
create index if not exists idx_book_external_ids_isbn10 on book_external_ids(provider_isbn10) where provider_isbn10 is not null;
create index if not exists idx_book_external_ids_asin on book_external_ids(provider_asin) where provider_asin is not null;

-- Table and column comments for book_external_ids
comment on table book_external_ids is 'External provider IDs and metadata for books (Google Books, Amazon, OpenLibrary, etc.)';
comment on column book_external_ids.external_id is 'Provider-specific ID (e.g., Google Books ID from JSON id field)';
comment on column book_external_ids.source is 'Provider name: GOOGLE_BOOKS, OPEN_LIBRARY, AMAZON, etc.';
comment on column book_external_ids.provider_isbn10 is 'ISBN-10 from volumeInfo.industryIdentifiers[type=ISBN_10]';
comment on column book_external_ids.provider_isbn13 is 'ISBN-13 from volumeInfo.industryIdentifiers[type=ISBN_13]';
comment on column book_external_ids.info_link is 'Provider info page from volumeInfo.infoLink';
comment on column book_external_ids.preview_link is 'Preview page from volumeInfo.previewLink';
comment on column book_external_ids.web_reader_link is 'Web reader from accessInfo.webReaderLink';
comment on column book_external_ids.canonical_volume_link is 'Canonical link from volumeInfo.canonicalVolumeLink';
comment on column book_external_ids.average_rating is 'Provider rating from volumeInfo.averageRating';
comment on column book_external_ids.ratings_count is 'Rating count from volumeInfo.ratingsCount';
comment on column book_external_ids.viewability is 'Access level from accessInfo.viewability (FULL, PARTIAL, NO_PAGES)';
comment on column book_external_ids.print_type is 'Content type from volumeInfo.printType (BOOK, MAGAZINE)';
comment on column book_external_ids.maturity_rating is 'Content rating from volumeInfo.maturityRating';
comment on column book_external_ids.saleability is 'Sale status from saleInfo.saleability';

-- ============================================================================
-- WORK CLUSTERING SYSTEM (replaces edition system)
-- ============================================================================

-- Work clusters table - groups books that are different editions of the same work
create table if not exists work_clusters (
  id uuid primary key default gen_random_uuid(),

  -- ISBN-based clustering
  isbn_prefix text, -- First 9-11 digits of ISBN-13 (publisher/work identifier)

  -- External work identifiers
  oclc_work_id text,
  openlibrary_work_id text,
  goodreads_work_id text,
  google_canonical_id text, -- Extracted from canonicalVolumeLink

  -- Cluster metadata
  canonical_title text not null,
  canonical_author text,
  confidence_score float default 0.5 check (confidence_score >= 0 and confidence_score <= 1),
  cluster_method text not null, -- 'ISBN_PREFIX', 'EXTERNAL_ID', 'ML_SIMILARITY', 'MANUAL'

  -- Statistics
  member_count integer default 0,
  constraint check_reasonable_member_count check (member_count <= 100),

  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

-- Unique constraints for external identifiers
create unique index if not exists uq_work_clusters_isbn_prefix
  on work_clusters(isbn_prefix) where isbn_prefix is not null;
create unique index if not exists uq_work_clusters_oclc
  on work_clusters(oclc_work_id) where oclc_work_id is not null;
create unique index if not exists uq_work_clusters_openlibrary
  on work_clusters(openlibrary_work_id) where openlibrary_work_id is not null;
create unique index if not exists uq_work_clusters_goodreads
  on work_clusters(goodreads_work_id) where goodreads_work_id is not null;
create unique index if not exists uq_work_clusters_google
  on work_clusters(google_canonical_id) where google_canonical_id is not null;

-- Indexes for lookups
create index if not exists idx_work_clusters_method on work_clusters(cluster_method);
create index if not exists idx_work_clusters_confidence on work_clusters(confidence_score desc);

comment on table work_clusters is 'Groups books that are different editions of the same work';
comment on column work_clusters.isbn_prefix is 'First 9-11 digits of ISBN-13 identifying publisher and work';
comment on column work_clusters.cluster_method is 'How this cluster was created: ISBN_PREFIX, EXTERNAL_ID, ML_SIMILARITY, MANUAL';

-- Work cluster membership
create table if not exists work_cluster_members (
  cluster_id uuid not null references work_clusters(id) on delete cascade,
  book_id uuid not null references books(id) on delete cascade,
  is_primary boolean default false, -- The "best" edition in this cluster
  confidence float default 0.5 check (confidence >= 0 and confidence <= 1),
  join_reason text not null, -- 'ISBN_PREFIX', 'OCLC_MATCH', 'OPENLIBRARY_MATCH', etc.
  joined_at timestamptz not null default now(),
  primary key (cluster_id, book_id)
);

create index if not exists idx_work_cluster_members_book on work_cluster_members(book_id);
create index if not exists idx_work_cluster_members_primary
  on work_cluster_members(cluster_id, is_primary) where is_primary = true;

comment on table work_cluster_members is 'Links books to their work clusters with confidence scoring';
comment on column work_cluster_members.is_primary is 'Marks the best/primary edition in this cluster';
comment on column work_cluster_members.join_reason is 'Why this book was added to this cluster';

-- View to easily see book editions
-- Postgres does not allow CREATE OR REPLACE VIEW to add/drop/reorder columns.
-- To keep startup idempotent and allow schema evolution, drop then recreate.
drop view if exists book_work_editions;
create view book_work_editions (
  book_id,
  title,
  isbn13,
  work_title,
  related_book_id,
  related_title,
  related_isbn13,
  related_publisher,
  confidence,
  cluster_method
) as
select
  b1.id as book_id,
  b1.title,
  b1.isbn13,
  wc.canonical_title as work_title,
  b2.id as related_book_id,
  b2.title as related_title,
  b2.isbn13 as related_isbn13,
  b2.publisher as related_publisher,
  wcm1.confidence,
  wc.cluster_method
from work_cluster_members wcm1
join work_clusters wc on wc.id = wcm1.cluster_id
join work_cluster_members wcm2 on wcm1.cluster_id = wcm2.cluster_id
join books b1 on b1.id = wcm1.book_id
join books b2 on b2.id = wcm2.book_id
where wcm1.book_id != wcm2.book_id;

comment on view book_work_editions is 'Shows all editions of books in the same work cluster';

-- Tags / qualifiers --------------------------------------------------------

create table if not exists book_tags (
  id text primary key, -- NanoID (10 chars)
  key text not null check (
    key = lower(regexp_replace(btrim(key), '[[:space:]]+', '_', 'g'))
    and key <> ''
  ), -- canonical tag key (e.g., 'nyt_bestseller')
  display_name text, -- human friendly label (e.g., 'NYT Bestseller')
  tag_type text not null default 'QUALIFIER', -- QUALIFIER, PROVIDER_TAG, USER_TAG, etc.
  description text,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  unique (key)
);

create table if not exists book_tag_equivalents (
  id text primary key, -- NanoID (10 chars)
  canonical_tag_id text not null references book_tags(id) on delete cascade,
  equivalent_tag_id text not null references book_tags(id) on delete cascade,
  created_at timestamptz not null default now(),
  unique (canonical_tag_id, equivalent_tag_id)
);

create table if not exists book_tag_assignments (
  id text primary key, -- NanoID (12 chars)
  book_id uuid not null references books(id) on delete cascade,
  tag_id text not null references book_tags(id) on delete cascade,
  source text not null check (source = btrim(source) and source <> ''), -- 'SEARCH_QUERY', 'PROVIDER', 'CURATION', etc.
  confidence numeric, -- optional 0..1 confidence score
  metadata jsonb, -- optional extra data (e.g., original phrase, provider payload snippet)
  created_at timestamptz not null default now(),
  unique (book_id, tag_id)
);

create index if not exists idx_book_tag_equivalents_canonical on book_tag_equivalents(canonical_tag_id);
create index if not exists idx_book_tag_equivalents_equivalent on book_tag_equivalents(equivalent_tag_id);
create index if not exists idx_book_tag_assignments_book_id on book_tag_assignments(book_id);
create index if not exists idx_book_tag_assignments_tag_id on book_tag_assignments(tag_id);

comment on table book_tags is 'Canonical tags/qualifiers applied to books (e.g., NYT bestseller, award winner).';
comment on column book_tags.key is 'Stable key used across services (snake_case).';
comment on column book_tags.tag_type is 'Tag classification: QUALIFIER, PROVIDER_TAG, USER_TAG, etc.';

comment on table book_tag_equivalents is 'Pairs of tags that should be treated as equivalents/aliases.';
comment on column book_tag_equivalents.canonical_tag_id is 'Primary tag in an equivalence relationship.';
comment on column book_tag_equivalents.equivalent_tag_id is 'Alias tag that resolves to the canonical tag.';

comment on table book_tag_assignments is 'Assignments linking canonical books to tags with source + optional metadata.';
comment on column book_tag_assignments.source is 'Most recent origin of the tag assignment (SEARCH_QUERY, PROVIDER, CURATION, etc).';
comment on column book_tag_assignments.metadata is 'Additional context, e.g., original query text or provider payload snippet.';

-- Author-domain DDL split into a dedicated module for maintainability.
\ir schema/author-domain.sql


-- Store the full raw JSON response for debugging/reprocessing
-- Also serves as contributing sources tracking (presence of a record = contributed data)
create table if not exists book_raw_data (
  id text primary key, -- NanoID (10 chars)
  book_id uuid not null references books(id) on delete cascade,
  raw_json_response jsonb not null, -- Complete API response as JSONB
  source text not null, -- 'GOOGLE_BOOKS', 'OPEN_LIBRARY', etc.
  fetched_at timestamptz not null default now(), -- When we fetched this data
  contributed_at timestamptz not null default now(), -- When this source contributed to the book
  created_at timestamptz not null default now(),
  unique(book_id, source)
);

create index if not exists idx_book_raw_data_book_id on book_raw_data(book_id);
create index if not exists idx_book_raw_data_source on book_raw_data(source);

-- Table comments for book_raw_data
comment on table book_raw_data is 'Raw JSON responses from providers for debugging/reprocessing';
comment on column book_raw_data.raw_json_response is 'Complete API response as JSONB';
comment on column book_raw_data.source is 'Provider that supplied this data';
comment on column book_raw_data.contributed_at is 'When this source contributed to the book record';

-- Store dimensions separately (not all books have this)
create table if not exists book_dimensions (
  book_id uuid primary key references books(id) on delete cascade,
  height numeric, -- JSON: volumeInfo.dimensions.height (parsed from "23.00 cm")
  width numeric, -- JSON: volumeInfo.dimensions.width (if available)
  thickness numeric, -- JSON: volumeInfo.dimensions.thickness (if available)
  weight_grams numeric, -- From other sources if available
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

-- Table comments for book_dimensions
comment on table book_dimensions is 'Physical dimensions of books when available';
comment on column book_dimensions.height is 'Height in cm parsed from volumeInfo.dimensions.height';
comment on column book_dimensions.width is 'Width in cm parsed from volumeInfo.dimensions.width';
comment on column book_dimensions.thickness is 'Thickness in cm parsed from volumeInfo.dimensions.thickness';

-- Store various image URLs from providers and map them to our S3 copies
create table if not exists book_image_links (
  id text primary key, -- NanoID (10 chars)
  book_id uuid not null references books(id) on delete cascade,
  image_type text not null, -- JSON keys: 'smallThumbnail', 'thumbnail', 'small', 'medium', 'large', 'extraLarge'
  url text not null, -- JSON: volumeInfo.imageLinks.{imageType} - full external URL
  s3_image_path text, -- Internal S3 key when image has been persisted
  source text, -- 'GOOGLE_BOOKS', etc. - which API provided this image
  width integer, -- Image width in pixels (estimated or actual)
  height integer, -- Image height in pixels (estimated or actual)
  is_high_resolution boolean default false, -- True for extraLarge/large images
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  unique(book_id, image_type)
);

create index if not exists idx_book_image_links_book_id on book_image_links(book_id);

-- Table comments for book_image_links
comment on table book_image_links is 'Maps external image URLs to our S3-persisted copies';
comment on column book_image_links.image_type is 'Image size: smallThumbnail, thumbnail, small, medium, large, extraLarge';
comment on column book_image_links.url is 'External URL from volumeInfo.imageLinks';
comment on column book_image_links.width is 'Image width in pixels (estimated or detected)';
comment on column book_image_links.height is 'Image height in pixels (estimated or detected)';
comment on column book_image_links.is_high_resolution is 'True for high-resolution images (extraLarge, large)';


-- Unified collections table for categories, lists, and custom collections
-- Can represent: NYT bestseller lists, Google Books categories, custom user lists, etc.
create table if not exists book_collections (
  id                 text primary key, -- NanoID (8 chars for low volume)
  collection_type    text not null, -- 'CATEGORY', 'BESTSELLER_LIST', 'CUSTOM_LIST', 'TAG', etc.
  source             text not null, -- 'NYT', 'GOOGLE_BOOKS', 'USER', 'SYSTEM', etc.
  provider_list_id   text, -- External ID if from a provider
  provider_list_code text, -- External code/slug if from a provider
  display_name       text not null, -- JSON: volumeInfo.categories[] or list display name
  normalized_name    text, -- For deduplication and searching
  description        text, -- Description of the collection/category
  parent_collection_id text references book_collections(id) on delete set null, -- For hierarchical categories
  -- List-specific fields (null for categories)
  bestsellers_date   date,
  published_date     date,
  updated_frequency  text,
  -- Metadata
  is_public          boolean default true,
  is_active          boolean default true,
  sort_order         integer,
  raw_data_json      jsonb, -- Raw data from provider if applicable
  created_at         timestamptz not null default now(),
  updated_at         timestamptz not null default now()
);

create unique index if not exists uq_book_collections_source_code_date
  on book_collections (source, provider_list_code, published_date);

create index if not exists idx_book_collections_type on book_collections(collection_type);
create index if not exists idx_book_collections_name on book_collections(display_name);
create index if not exists idx_book_collections_normalized on book_collections(normalized_name);
create unique index if not exists uq_book_collections_category
  on book_collections (collection_type, source, normalized_name)
  where collection_type = 'CATEGORY' and normalized_name is not null;
create index if not exists idx_book_collections_parent on book_collections(parent_collection_id);
create index if not exists idx_book_collections_latest
  on book_collections (source, provider_list_code, published_date desc)
  where provider_list_code is not null;

-- Join table linking books to collections (categories, lists, etc.)
create table if not exists book_collections_join (
  id                text primary key, -- NanoID (12 chars via IdGenerator.generateLong() - high volume)
  collection_id     text not null references book_collections(id) on delete cascade,
  book_id           uuid not null references books(id) on delete cascade,
  -- Position/ranking in the collection (for ordered lists)
  position          integer, -- Rank in bestseller list or sort order
  -- List-specific metadata
  weeks_on_list     integer, -- For bestseller lists
  rank_last_week    integer, -- Previous rank for trending
  peak_position     integer, -- Best rank achieved
  -- Provider references if the book was added via external data
  provider_isbn13   text, -- ISBN from the list provider
  provider_isbn10   text, -- ISBN-10 from the list provider
  provider_book_ref text, -- Provider's reference/ID for this book
  -- Custom metadata (ratings, notes, etc.)
  user_rating       numeric, -- User's personal rating if custom list
  notes             text, -- User notes about why book is in this collection
  -- Raw data from provider if applicable
  raw_item_json     jsonb, -- Complete list item data from provider
  -- Timestamps
  added_at          timestamptz not null default now(),
  created_at        timestamptz not null default now(),
  updated_at        timestamptz not null default now(),
  unique (collection_id, book_id)
);

create unique index if not exists uq_book_collections_join_position
  on book_collections_join (collection_id, position)
  where position is not null;

create index if not exists idx_book_collections_join_collection on book_collections_join(collection_id);
create index if not exists idx_book_collections_join_book on book_collections_join(book_id);
create index if not exists idx_book_collections_join_added on book_collections_join(collection_id, added_at desc);

-- Table comments for book_collections
comment on table book_collections is 'Unified table for categories, bestseller lists, and custom collections';
comment on column book_collections.collection_type is 'Type: CATEGORY, BESTSELLER_LIST, CUSTOM_LIST, TAG';
comment on column book_collections.source is 'Origin: NYT, GOOGLE_BOOKS, USER, SYSTEM';
comment on column book_collections.display_name is 'Display name from volumeInfo.categories[] or list name';

-- Table comments for book_collections_join
comment on table book_collections_join is 'Many-to-many relationship between books and collections';
comment on column book_collections_join.position is 'Rank in bestseller list or sort order';
comment on column book_collections_join.weeks_on_list is 'Number of weeks on bestseller list';

-- ============================================================================
-- SEARCH FUNCTIONALITY
-- ============================================================================

-- Materialized view for optimized book search
-- Denormalizes books with authors for fast full-text and fuzzy search
create materialized view if not exists book_search_view as
select
  b.id as book_id,
  b.title,
  b.subtitle,
  b.slug,
  b.isbn13,
  b.isbn10,
  b.published_date,
  b.publisher,
  b.language,
  b.page_count,
  string_agg(a.name, ', ' order by ba.position) as authors,
  -- Combined search vector with weights:
  -- A: title, author names (highest priority)
  -- B: subtitle (high priority)
  -- C: publisher (medium priority)
  -- D: description (low priority)
  (
    setweight(to_tsvector('english', coalesce(b.title, '')), 'A') ||
    setweight(to_tsvector('english', coalesce(string_agg(a.name, ' ' order by ba.position), '')), 'A') ||
    setweight(to_tsvector('english', coalesce(b.subtitle, '')), 'B') ||
    setweight(to_tsvector('english', coalesce(b.publisher, '')), 'C') ||
    setweight(to_tsvector('english', coalesce(b.description, '')), 'D')
  ) as search_vector,
  -- Searchable text for trigram similarity (fuzzy matching)
  lower(
    coalesce(b.title, '') || ' ' ||
    coalesce(b.subtitle, '') || ' ' ||
    coalesce(string_agg(a.name, ' ' order by ba.position), '') || ' ' ||
    coalesce(b.publisher, '')
  ) as searchable_text
from books b
left join book_authors_join ba on b.id = ba.book_id
left join authors a on ba.author_id = a.id
group by b.id;

-- Indexes for the materialized view
create index if not exists idx_book_search_view_search_vector
  on book_search_view using gin (search_vector);

create index if not exists idx_book_search_view_searchable_text
  on book_search_view using gin (searchable_text gin_trgm_ops);

-- Unique index for concurrent refresh
create unique index if not exists idx_book_search_view_book_id
  on book_search_view (book_id);

-- Function to refresh the search view (call after bulk updates)
drop function if exists refresh_book_search_view();
create or replace function refresh_book_search_view()
returns void
language plpgsql
as $$
begin
  -- Non-concurrent so it can run inside the initializer's transaction and within a function
  refresh materialized view book_search_view;
end;
$$;

-- Main search function combining multiple strategies with cluster deduplication
create or replace function search_books(
  search_query text,
  max_results integer default 20
)
returns table (
  book_id uuid,
  title text,
  subtitle text,
  authors text,
  isbn13 text,
  isbn10 text,
  published_date date,
  publisher text,
  relevance_score float,
  match_type text,
  edition_count integer,
  cluster_id uuid
) as $$
begin
  return query
  with
  -- Strategy 1: Exact title matches (highest priority)
  exact_matches as (
    select
      b.book_id,
      b.title,
      b.subtitle,
      b.authors,
      b.isbn13,
      b.isbn10,
      b.published_date,
      b.publisher,
      1.0::float as relevance_score,
      'exact_title'::text as match_type,
      coalesce(wc.member_count, 1) as edition_count,
      wc.id as cluster_id,
      -- Add priority for selecting primary edition
      coalesce(wcm.is_primary, false) as is_primary,
      -- Quality metrics for tie-breaking
      coalesce(
        (
          select coalesce(bil.is_high_resolution, false)
          from book_image_links bil
          where bil.book_id = b.book_id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc
          limit 1
        ),
        false
      ) as has_high_res_cover
    from book_search_view b
    left join work_cluster_members wcm on b.book_id = wcm.book_id
    left join work_clusters wc on wcm.cluster_id = wc.id
    where lower(b.title) = lower(search_query)
    order by
      coalesce(wcm.is_primary, false) desc,
      has_high_res_cover desc,
      b.published_date desc nulls last,
      lower(b.title),
      b.book_id
    limit 50  -- Get more results for deduplication
  ),
  -- Strategy 2: Full-text search with ranking
  fulltext_matches as (
    select
      b.book_id,
      b.title,
      b.subtitle,
      b.authors,
      b.isbn13,
      b.isbn10,
      b.published_date,
      b.publisher,
      ts_rank(b.search_vector, plainto_tsquery('english', search_query))::float as relevance_score,
      'fulltext'::text as match_type,
      coalesce(wc.member_count, 1) as edition_count,
      wc.id as cluster_id,
      coalesce(wcm.is_primary, false) as is_primary,
      coalesce(
        (
          select coalesce(bil.is_high_resolution, false)
          from book_image_links bil
          where bil.book_id = b.book_id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc
          limit 1
        ),
        false
      ) as has_high_res_cover
    from book_search_view b
    left join work_cluster_members wcm on b.book_id = wcm.book_id
    left join work_clusters wc on wcm.cluster_id = wc.id
    where b.search_vector @@ plainto_tsquery('english', search_query)
      and b.book_id not in (select em.book_id from exact_matches em)
    order by
      relevance_score desc,
      coalesce(wcm.is_primary, false) desc,
      has_high_res_cover desc,
      b.published_date desc nulls last,
      lower(b.title),
      b.book_id
    limit 100  -- Get more results for deduplication
  ),
  -- Strategy 3: Fuzzy matches for typo tolerance
  fuzzy_matches as (
    select
      b.book_id,
      b.title,
      b.subtitle,
      b.authors,
      b.isbn13,
      b.isbn10,
      b.published_date,
      b.publisher,
      similarity(b.searchable_text, lower(search_query))::float * 0.8 as relevance_score,
      'fuzzy'::text as match_type,
      coalesce(wc.member_count, 1) as edition_count,
      wc.id as cluster_id,
      coalesce(wcm.is_primary, false) as is_primary,
      coalesce(
        (
          select coalesce(bil.is_high_resolution, false)
          from book_image_links bil
          where bil.book_id = b.book_id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc
          limit 1
        ),
        false
      ) as has_high_res_cover
    from book_search_view b
    left join work_cluster_members wcm on b.book_id = wcm.book_id
    left join work_clusters wc on wcm.cluster_id = wc.id
    where b.searchable_text % lower(search_query)
      and b.book_id not in (
        select em.book_id from exact_matches em
        union select fm.book_id from fulltext_matches fm
      )
    order by
      relevance_score desc,
      coalesce(wcm.is_primary, false) desc,
      has_high_res_cover desc,
      b.published_date desc nulls last,
      lower(b.title),
      b.book_id
    limit 50  -- Get more results for deduplication
  ),
  -- Combine all results
  all_matches as (
    select * from exact_matches
    union all
    select * from fulltext_matches
    union all
    select * from fuzzy_matches
  ),
  -- Deduplicate by cluster: keep only the best book per cluster
  deduplicated as (
    select distinct on (coalesce(am.cluster_id, am.book_id))
      am.book_id,
      am.title,
      am.subtitle,
      am.authors,
      am.isbn13,
      am.isbn10,
      am.published_date,
      am.publisher,
      am.relevance_score,
      am.match_type,
      am.edition_count,
      am.cluster_id
    from all_matches am
    order by
      coalesce(am.cluster_id, am.book_id),  -- Group by cluster (or book if no cluster)
      am.relevance_score desc,               -- Prefer higher relevance
      am.is_primary desc,                    -- Prefer primary edition
      am.has_high_res_cover desc,            -- Prefer better cover quality
      am.published_date desc nulls last      -- Prefer newer editions
  )
  -- Final results ordered by relevance
  select * from deduplicated
  order by
    relevance_score desc,
    lower(title),
    coalesce(cluster_id, book_id)
  limit max_results;
end;
$$ language plpgsql;

-- ISBN search function for barcode scanning
create or replace function search_by_isbn(isbn_query text)
returns table (
  book_id uuid,
  title text,
  subtitle text,
  authors text,
  isbn13 text,
  isbn10 text,
  published_date date,
  publisher text
) as $$
declare
  clean_isbn text;
begin
  -- Remove any non-digit characters from ISBN
  clean_isbn := regexp_replace(isbn_query, '[^0-9]', '', 'g');

  return query
  select
    b.id as book_id,
    b.title,
    b.subtitle,
    string_agg(a.name, ', ' order by ba.position) as authors,
    b.isbn13,
    b.isbn10,
    b.published_date,
    b.publisher
  from books b
  left join book_authors_join ba on b.id = ba.book_id
  left join authors a on ba.author_id = a.id
  where
    regexp_replace(b.isbn13, '[^0-9]', '', 'g') = clean_isbn
    or regexp_replace(b.isbn10, '[^0-9]', '', 'g') = clean_isbn
  group by b.id
  limit 1;
end;
$$ language plpgsql;

-- Author search function
create or replace function search_authors(
  search_query text,
  max_results integer default 20
)
returns table (
  author_id text,
  author_name text,
  book_count bigint,
  relevance_score float
) as $$
begin
  return query
  with author_matches as (
    select
      a.id as author_id,
      a.name as author_name,
      count(distinct ba.book_id) as book_count,
      case
        when lower(a.name) = lower(search_query) then 1.0
        when lower(a.name) like lower(search_query) || '%' then 0.9
        when a.search_vector @@ plainto_tsquery('english', search_query) then
          ts_rank(a.search_vector, plainto_tsquery('english', search_query))::float
        else similarity(lower(a.name), lower(search_query)) * 0.7
      end as relevance_score
    from authors a
    left join book_authors_join ba on a.id = ba.author_id
    where
      lower(a.name) like '%' || lower(search_query) || '%'
      or a.search_vector @@ plainto_tsquery('english', search_query)
      or lower(a.name) % lower(search_query)
    group by a.id, a.name, a.search_vector
  )
  select * from author_matches
  order by
    relevance_score desc,
    book_count desc,
    lower(author_name),
    author_id
  limit max_results;
end;
$$ language plpgsql;

-- Comments for search components
comment on materialized view book_search_view is 'Denormalized view optimized for full-text and fuzzy search';
comment on function search_books is 'Smart search combining exact, full-text, and fuzzy matching strategies';
comment on function search_by_isbn is 'Search for books by ISBN-10 or ISBN-13, handles various formats';
comment on function search_authors is 'Search for authors with relevance ranking and book count';
comment on function refresh_book_search_view is 'Refresh the search materialized view after bulk updates';

-- ============================================================================
-- SLUG MANAGEMENT
-- ============================================================================

-- Function to find book by slug or ID (supports legacy URLs)
create or replace function find_book_by_slug_or_id(identifier text)
returns table (
  book_id uuid,
  title text,
  subtitle text,
  slug text,
  authors text,
  isbn13 text,
  isbn10 text,
  published_date date,
  publisher text
) as $$
begin
  -- First try to find by slug
  return query
  select
    b.id as book_id,
    b.title,
    b.subtitle,
    b.slug,
    string_agg(a.name, ', ' order by ba.position) as authors,
    b.isbn13,
    b.isbn10,
    b.published_date,
    b.publisher
  from books b
  left join book_authors_join ba on b.id = ba.book_id
  left join authors a on ba.author_id = a.id
  where b.slug = identifier
  group by b.id
  limit 1;

  -- If not found, try as UUID (legacy support)
  if not found then
    return query
    select
      b.id as book_id,
      b.title,
      b.subtitle,
      b.slug,
      string_agg(a.name, ', ' order by ba.position) as authors,
      b.isbn13,
      b.isbn10,
      b.published_date,
      b.publisher
    from books b
    left join book_authors_join ba on b.id = ba.book_id
    left join authors a on ba.author_id = a.id
    where b.id::text = identifier
    group by b.id
    limit 1;
  end if;
end;
$$ language plpgsql;

-- Function to ensure slug uniqueness by appending counter if needed
create or replace function ensure_unique_slug(base_slug text)
returns text as $$
declare
  final_slug text;
  counter integer := 1;
begin
  final_slug := base_slug;

  -- Check if slug exists
  while exists(select 1 from books where slug = final_slug) loop
    final_slug := base_slug || '-' || counter;
    counter := counter + 1;
  end loop;

  return final_slug;
end;
$$ language plpgsql;

comment on function find_book_by_slug_or_id is 'Find book by SEO slug or UUID, supports legacy URL patterns';
comment on function ensure_unique_slug is 'Generate unique slug by appending counter if base slug already exists';

-- ============================================================================
-- RECOMMENDATIONS CACHE
-- ============================================================================

-- Cache for book-to-book recommendations
create table if not exists book_recommendations (
  id text primary key, -- NanoID (10 chars via IdGenerator.generate())
  source_book_id uuid not null references books(id) on delete cascade,
  recommended_book_id uuid not null references books(id) on delete cascade,
  source text not null, -- 'GOOGLE_SIMILAR', 'SAME_AUTHOR', 'SAME_CATEGORY', 'AI_GENERATED'
  score float check (score >= 0 and score <= 1), -- Relevance score 0.0-1.0
  reason text, -- Why this was recommended
  generated_at timestamptz not null default now(),
  expires_at timestamptz default (now() + interval '30 days'),
  created_at timestamptz not null default now(),
  unique(source_book_id, recommended_book_id, source)
);

create index if not exists idx_book_recommendations_source on book_recommendations(source_book_id);
create index if not exists idx_book_recommendations_target on book_recommendations(recommended_book_id);
create index if not exists idx_book_recommendations_expires on book_recommendations(expires_at) where expires_at is not null;

comment on table book_recommendations is 'Cached book-to-book recommendations from various sources';
comment on column book_recommendations.source is 'Origin of recommendation: GOOGLE_SIMILAR, SAME_AUTHOR, SAME_CATEGORY, AI_GENERATED';
comment on column book_recommendations.score is 'Relevance score from 0.0 (weak) to 1.0 (strong)';
comment on column book_recommendations.expires_at is 'When to refresh this recommendation';

-- ============================================================================
-- ASYNC BACKFILL INFRASTRUCTURE
-- ============================================================================
-- NOTE: Backfill queue now uses BackfillQueueService (in-memory queue)
-- instead of database table. See BackfillQueueService.java and BackfillCoordinator.java

-- Transactional outbox pattern for WebSocket events
-- Ensures events are reliably delivered even if WebSocket publish fails
create table if not exists events_outbox (
  event_id uuid primary key default gen_random_uuid(),
  topic text not null, -- WebSocket topic path: /topic/search.{id}, /topic/book.{id}
  payload jsonb not null, -- Event data as JSON
  created_at timestamptz not null default now(),
  sent_at timestamptz, -- NULL until successfully published to WebSocket
  retry_count int not null default 0
);

create index if not exists idx_events_outbox_unsent on events_outbox(created_at) where sent_at is null;
create index if not exists idx_events_outbox_sent on events_outbox(sent_at desc) where sent_at is not null;

comment on table events_outbox is 'Transactional outbox for reliable WebSocket event delivery';
comment on column events_outbox.topic is 'WebSocket destination topic (e.g., /topic/book.{bookId})';
comment on column events_outbox.sent_at is 'When event was successfully published (NULL = pending)';

-- Slug redirect table to handle book slug changes over time
-- When a book's slug changes (due to title/author updates), old URLs redirect to new slug
create table if not exists book_slug_redirect (
  old_slug text primary key,
  book_id uuid not null references books(id) on delete cascade,
  created_at timestamptz not null default now()
);

create index if not exists idx_book_slug_redirect_book_id on book_slug_redirect(book_id);

comment on table book_slug_redirect is 'Maps old book slugs to current book IDs for permanent redirects';
comment on column book_slug_redirect.old_slug is 'Previous SEO slug that no longer matches book.slug';
comment on column book_slug_redirect.book_id is 'Current book ID (use to look up current slug)';

-- ============================================================================
-- IMAGE METADATA EXTENSION
-- ============================================================================

-- Add metadata columns to book_image_links for completeness
alter table book_image_links
add column if not exists width integer,
add column if not exists height integer,
add column if not exists file_size_bytes integer,
add column if not exists is_high_resolution boolean,
add column if not exists updated_at timestamptz,
add column if not exists s3_uploaded_at timestamptz,
add column if not exists download_error text;

update book_image_links
set updated_at = coalesce(updated_at, created_at, now())
where updated_at is null;

alter table book_image_links
alter column updated_at set default now();

update book_image_links
set s3_uploaded_at = coalesce(s3_uploaded_at, created_at, now())
where s3_image_path is not null
  and s3_uploaded_at is null;

delete from book_image_links
where lower(image_type) in ('preferred', 'fallback', 's3')
   or lower(url) like '%placeholder-book-cover.svg%'
   or url !~* '^https?://';

do $$
begin
  if not exists (
    select 1
    from pg_constraint c
    join pg_class t on t.oid = c.conrelid
    where t.relname = 'book_image_links'
      and c.conname = 'check_book_image_links_disallowed_types'
  ) then
    alter table book_image_links
      add constraint check_book_image_links_disallowed_types
      check (lower(image_type) <> all (array['preferred', 'fallback', 's3'])) not valid;
  end if;
end
$$;

do $$
begin
  if not exists (
    select 1
    from pg_constraint c
    join pg_class t on t.oid = c.conrelid
    where t.relname = 'book_image_links'
      and c.conname = 'check_book_image_links_http_url'
  ) then
    alter table book_image_links
      add constraint check_book_image_links_http_url
      check (url ~* '^https?://') not valid;
  end if;
end
$$;

do $$
begin
  if not exists (
    select 1
    from pg_constraint c
    join pg_class t on t.oid = c.conrelid
    where t.relname = 'book_image_links'
      and c.conname = 'check_book_image_links_no_placeholder'
  ) then
    alter table book_image_links
      add constraint check_book_image_links_no_placeholder
      check (position('placeholder-book-cover.svg' in lower(url)) = 0) not valid;
  end if;
end
$$;

alter table book_image_links validate constraint check_book_image_links_disallowed_types;
alter table book_image_links validate constraint check_book_image_links_http_url;
alter table book_image_links validate constraint check_book_image_links_no_placeholder;

create or replace function set_book_image_links_updated_at()
returns trigger as $$
begin
  new.updated_at := now();
  return new;
end;
$$ language plpgsql;

drop trigger if exists book_image_links_set_updated_at on book_image_links;
create trigger book_image_links_set_updated_at
  before update on book_image_links
  for each row
  execute function set_book_image_links_updated_at();

comment on column book_image_links.width is 'Image width in pixels';
comment on column book_image_links.height is 'Image height in pixels';
comment on column book_image_links.file_size_bytes is 'File size in bytes';
comment on column book_image_links.is_high_resolution is 'True if image is high quality (>300px width or >100KB)';
comment on column book_image_links.s3_image_path is 'S3 path to our persisted copy of this image';
comment on column book_image_links.updated_at is 'Timestamp of the most recent mutation to this image link row';
comment on column book_image_links.s3_uploaded_at is 'When we uploaded this image to S3';
comment on column book_image_links.download_error is 'Error message if download failed';

-- ============================================================================
-- AUTHOR NORMALIZATION
-- ============================================================================

-- Comprehensive author name normalization for deduplication
create or replace function normalize_author_name(author_name text)
returns text as $$
declare
  normalized text;
begin
  -- Return NULL for NULL input
  if author_name is null or trim(author_name) = '' then
    return null;
  end if;

  normalized := author_name;

  -- Convert to lowercase
  normalized := lower(normalized);

  -- Remove accents and diacritics
  normalized := translate(normalized,
    'àáäâãåèéëêìíïîòóöôõùúüûñçğışÀÁÄÂÃÅÈÉËÊÌÍÏÎÒÓÖÔÕÙÚÜÛÑÇĞİŞ',
    'aaaaaaeeeeiiiiooooouuuuncgisAAAAAEEEEIIIIOOOOOUUUUNCGIS');

  -- Handle common suffixes and titles
  normalized := regexp_replace(normalized, '\s+jr\.?(?:\s|$)', ' jr', 'gi');
  normalized := regexp_replace(normalized, '\s+sr\.?(?:\s|$)', ' sr', 'gi');
  normalized := regexp_replace(normalized, '\s+ph\.?d\.?(?:\s|$)', ' phd', 'gi');
  normalized := regexp_replace(normalized, '\s+m\.?d\.?(?:\s|$)', ' md', 'gi');

  -- Remove possessives
  normalized := regexp_replace(normalized, '''s(?:\s|$)', '', 'g');

  -- Handle corporate suffixes
  normalized := regexp_replace(normalized, '\s+inc\.?(?:\s|$)', ' inc', 'gi');
  normalized := regexp_replace(normalized, '\s+corp\.?(?:\s|$)', ' corp', 'gi');
  normalized := regexp_replace(normalized, '\s+ltd\.?(?:\s|$)', ' ltd', 'gi');
  normalized := regexp_replace(normalized, '\s+llc\.?(?:\s|$)', ' llc', 'gi');

  -- Remove editorial annotations
  normalized := regexp_replace(normalized, '\[from old catalog\]', '', 'gi');
  normalized := regexp_replace(normalized, '\(editor\)', '', 'gi');
  normalized := regexp_replace(normalized, '\(author\)', '', 'gi');

  -- Normalize punctuation and spaces
  normalized := regexp_replace(normalized, '[^a-z0-9\s]', ' ', 'g');
  normalized := regexp_replace(normalized, '\s+', ' ', 'g');
  normalized := trim(normalized);

  return normalized;
end;
$$ language plpgsql immutable;

comment on function normalize_author_name is 'Normalizes author names for deduplication: handles accents, suffixes, corporate names';

-- Function to merge duplicate authors with same normalized name
create or replace function merge_duplicate_authors()
returns table (
  groups_found integer,
  authors_merged integer
) as $$
declare
  rec record;
  primary_author_id text;
  merge_count integer := 0;
  group_count integer := 0;
begin
  -- Find groups of authors with same normalized name
  for rec in
    select
      normalized_name,
      array_agg(id order by created_at, id) as author_ids
    from authors
    where normalized_name is not null
    group by normalized_name
    having count(*) > 1
  loop
    group_count := group_count + 1;
    primary_author_id := rec.author_ids[1];

    -- Update all book references to point to primary author
    update book_authors_join
    set author_id = primary_author_id
    where author_id = any(rec.author_ids[2:])
      and not exists (
        select 1 from book_authors_join baj2
        where baj2.book_id = book_authors_join.book_id
          and baj2.author_id = primary_author_id
      );

    -- Merge author external IDs
    update author_external_ids
    set author_id = primary_author_id
    where author_id = any(rec.author_ids[2:])
      and not exists (
        select 1 from author_external_ids aei2
        where aei2.source = author_external_ids.source
          and aei2.external_id = author_external_ids.external_id
          and aei2.author_id = primary_author_id
      );

    -- Delete duplicate book-author entries
    delete from book_authors_join
    where author_id = any(rec.author_ids[2:]);

    -- Delete duplicate authors (keep primary)
    delete from authors
    where id = any(rec.author_ids[2:]);

    merge_count := merge_count + array_length(rec.author_ids, 1) - 1;
  end loop;

  return query select group_count, merge_count;
end;
$$ language plpgsql;

comment on function merge_duplicate_authors is 'Merges authors with identical normalized names, preserving relationships';

-- ============================================================================
-- MIGRATION HELPERS
-- ============================================================================

-- PostgreSQL function to generate slug from title and authors
-- Used during migration when Java SlugGenerator is not available
create or replace function generate_slug(title text, author_name text default null)
returns text as $$
declare
  slug text;
begin
  -- Start with title
  slug := lower(title);

  -- Basic replacements
  slug := regexp_replace(slug, '&', 'and', 'g');

  -- Remove accents/diacritics
  slug := translate(slug,
    'àáäâãåèéëêìíïîòóöôõùúüûñçğışÀÁÄÂÃÅÈÉËÊÌÍÏÎÒÓÖÔÕÙÚÜÛÑÇĞİŞ',
    'aaaaaaeeeeiiiiooooouuuuncgisAAAAAEEEEIIIIOOOOOUUUUNCGIS');

  -- Keep only alphanumeric and spaces/hyphens
  slug := regexp_replace(slug, '[^a-z0-9\s-]', '', 'g');

  -- Replace spaces with hyphens
  slug := regexp_replace(slug, '[\s_]+', '-', 'g');

  -- Remove multiple consecutive hyphens
  slug := regexp_replace(slug, '-+', '-', 'g');

  -- Trim hyphens from start/end
  slug := trim(both '-' from slug);

  -- Truncate title part to 60 chars at word boundary
  if length(slug) > 60 then
    slug := substring(slug from 1 for 60);
    -- Try to cut at last hyphen
    slug := regexp_replace(slug, '-[^-]*$', '');
  end if;

  -- Add author if provided
  if author_name is not null and author_name != '' then
    declare
      author_slug text;
    begin
      author_slug := lower(author_name);
      author_slug := regexp_replace(author_slug, '[^a-z0-9\s-]', '', 'g');
      author_slug := regexp_replace(author_slug, '[\s_]+', '-', 'g');
      author_slug := regexp_replace(author_slug, '-+', '-', 'g');
      author_slug := trim(both '-' from author_slug);

      -- Truncate author to 30 chars
      if length(author_slug) > 30 then
        author_slug := substring(author_slug from 1 for 30);
        author_slug := regexp_replace(author_slug, '-[^-]*$', '');
      end if;

      slug := slug || '-' || author_slug;
    end;
  end if;

  -- Final truncation to 100 chars
  if length(slug) > 100 then
    slug := substring(slug from 1 for 100);
    slug := regexp_replace(slug, '-[^-]*$', '');
  end if;

  -- Ensure it's not empty
  if slug = '' or slug is null then
    slug := 'book';
  end if;

  return slug;
end;
$$ language plpgsql;

-- Function to batch generate slugs for existing books
-- Call this after migration to populate slugs
create or replace function generate_all_book_slugs()
returns void as $$
declare
  book_record record;
  base_slug text;
  final_slug text;
  counter integer;
begin
  -- Loop through all books without slugs
  for book_record in
    select
      b.id,
      b.title,
      string_agg(a.name, ' ' order by ba.position) as first_author
    from books b
    left join book_authors_join ba on b.id = ba.book_id and ba.position = 0
    left join authors a on ba.author_id = a.id
    where b.slug is null or b.slug = ''
    group by b.id, b.title
  loop
    -- Generate base slug
    base_slug := generate_slug(book_record.title, book_record.first_author);
    final_slug := base_slug;
    counter := 1;

    -- Ensure uniqueness
    while exists(select 1 from books where slug = final_slug and id != book_record.id) loop
      counter := counter + 1;
      final_slug := base_slug || '-' || counter;
    end loop;

    -- Update the book with its slug
    update books set slug = final_slug where id = book_record.id;
  end loop;

  raise notice 'Generated slugs for all books';
end;
$$ language plpgsql;

comment on function generate_slug is 'Generate URL slug from title and optional author, used during migration';
comment on function generate_all_book_slugs is 'Batch generate slugs for all books that dont have one, used after data migration';

-- ============================================================================
-- WORK CLUSTERING FUNCTIONS
-- ============================================================================

-- Extract ISBN work prefix (publisher + title identifier)
create or replace function extract_isbn_work_prefix(isbn13 text)
returns text as $$
begin
  if isbn13 is null or length(isbn13) < 13 then
    return null;
  end if;

  -- Remove any non-digits
  isbn13 := regexp_replace(isbn13, '[^0-9]', '', 'g');

  if length(isbn13) != 13 then
    return null;
  end if;

  -- Return first 11 digits (groups editions from same publisher)
  return substring(isbn13, 1, 11);
end;
$$ language plpgsql immutable;

-- Cluster books by ISBN prefix
create or replace function cluster_books_by_isbn()
returns table (clusters_created integer, books_clustered integer) as $$
declare
  rec record;
  cluster_uuid uuid;
  clusters_count integer := 0;
  books_count integer := 0;
  book_uuid uuid;
  primary_title text;
begin
  for rec in
    select
      prefix,
      array_agg(book_id order by has_high_res desc, cover_area desc, published_date desc nulls last, lower(title)) as book_ids,
      count(*) as book_count
    from (
      select
        extract_isbn_work_prefix(b.isbn13) as prefix,
        b.id as book_id,
        b.title,
        b.published_date,
        coalesce(
          (
            select coalesce(bil.is_high_resolution, false)
            from book_image_links bil
            where bil.book_id = b.id
            order by coalesce(bil.is_high_resolution, false) desc,
                     coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                     bil.created_at desc
            limit 1
          ),
          false
        ) as has_high_res,
        coalesce(
          (
            select coalesce((bil.width::bigint * bil.height::bigint), 0)
            from book_image_links bil
            where bil.book_id = b.id
            order by coalesce(bil.is_high_resolution, false) desc,
                     coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                     bil.created_at desc
            limit 1
          ),
          0
        ) as cover_area
      from books b
      where b.isbn13 is not null
    ) candidate
    where prefix is not null
    group by prefix
    having count(*) > 1
  loop
    if rec.book_ids is null or array_length(rec.book_ids, 1) = 0 then
      continue;
    end if;

    -- Get the primary title from the first (best) book with fallback for NULL/empty
    select coalesce(nullif(title, ''), 'Untitled Book') into primary_title
    from books
    where id = rec.book_ids[1];

    -- Ensure primary_title is never NULL
    if primary_title is null or primary_title = '' then
      primary_title := 'Untitled Book';
    end if;

    select id into cluster_uuid
    from work_clusters
    where isbn_prefix = rec.prefix;

    if cluster_uuid is null then
      insert into work_clusters (isbn_prefix, canonical_title, confidence_score, cluster_method, member_count)
      values (rec.prefix, primary_title, 0.9, 'ISBN_PREFIX', rec.book_count)
      returning id into cluster_uuid;
      clusters_count := clusters_count + 1;
    else
      update work_clusters
      set canonical_title = primary_title,
          member_count = rec.book_count,
          updated_at = now()
      where id = cluster_uuid;
    end if;

    for i in 1..array_length(rec.book_ids, 1) loop
      book_uuid := rec.book_ids[i];

      insert into work_cluster_members (cluster_id, book_id, is_primary, confidence, join_reason)
      values (cluster_uuid, book_uuid, (i = 1), 0.9, 'ISBN_PREFIX')
      on conflict (cluster_id, book_id) do update set
        is_primary = excluded.is_primary,
        confidence = excluded.confidence,
        join_reason = excluded.join_reason,
        joined_at = now();

      books_count := books_count + 1;
    end loop;
  end loop;

  return query select clusters_count, books_count;
end;
$$ language plpgsql;

create or replace function cluster_single_book_by_isbn(target_book_id uuid)
returns void as $$
declare
  prefix text;
  cluster_uuid uuid;
  book_ids uuid[];
  primary_title text;
  book_count integer;
  i integer;
  current_book uuid;
begin
  select extract_isbn_work_prefix(isbn13) into prefix
  from books
  where id = target_book_id;

  if prefix is null then
    return;
  end if;

  select
    array_agg(book_id order by has_high_res desc, cover_area desc, published_date desc nulls last, lower(title)) as ordered_ids,
    count(*) as total_books
  into book_ids, book_count
  from (
    select
      b.id as book_id,
      b.title,
      b.published_date,
      coalesce(
        (
          select coalesce(bil.is_high_resolution, false)
          from book_image_links bil
          where bil.book_id = b.id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                   bil.created_at desc
          limit 1
        ),
        false
      ) as has_high_res,
      coalesce(
        (
          select coalesce((bil.width::bigint * bil.height::bigint), 0)
          from book_image_links bil
          where bil.book_id = b.id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                   bil.created_at desc
          limit 1
        ),
        0
      ) as cover_area
    from books b
    where extract_isbn_work_prefix(b.isbn13) = prefix
  ) ranked;

  if book_count is null or book_count < 2 then
    return;
  end if;

  -- Get primary title with fallback for NULL/empty
  select coalesce(nullif(title, ''), 'Untitled Book') into primary_title
  from books
  where id = book_ids[1];

  -- Ensure primary_title is never NULL
  if primary_title is null or primary_title = '' then
    primary_title := 'Untitled Book';
  end if;

  select id into cluster_uuid
  from work_clusters
  where isbn_prefix = prefix;

  if cluster_uuid is null then
    insert into work_clusters (isbn_prefix, canonical_title, confidence_score, cluster_method, member_count)
    values (prefix, primary_title, 0.9, 'ISBN_PREFIX', book_count)
    returning id into cluster_uuid;
  else
    update work_clusters
    set canonical_title = primary_title,
        member_count = book_count,
        updated_at = now()
    where id = cluster_uuid;
  end if;

  for i in 1..array_length(book_ids, 1) loop
    current_book := book_ids[i];

    insert into work_cluster_members (cluster_id, book_id, is_primary, confidence, join_reason)
    values (cluster_uuid, current_book, (i = 1), 0.9, 'ISBN_PREFIX')
    on conflict (cluster_id, book_id) do update set
      is_primary = excluded.is_primary,
      confidence = excluded.confidence,
      join_reason = excluded.join_reason,
      joined_at = now();
  end loop;
end;
$$ language plpgsql;

create or replace function cluster_single_book_by_google_id(target_book_id uuid)
returns void as $$
declare
  canonical_id text;
  cluster_uuid uuid;
  book_ids uuid[];
  book_count integer;
  primary_title text;
  i integer;
  current_book uuid;
begin
  select google_canonical_id into canonical_id
  from book_external_ids
  where book_id = target_book_id
    and google_canonical_id is not null
    and source = 'GOOGLE_BOOKS'
  limit 1;

  if canonical_id is not null then
    canonical_id := nullif(regexp_replace(canonical_id, '[[:cntrl:]]', '', 'g'), '');
  end if;

  if canonical_id is null then
    select canonical_volume_link into canonical_id
    from book_external_ids
    where book_id = target_book_id
      and canonical_volume_link is not null
      and source = 'GOOGLE_BOOKS'
    limit 1;

    if canonical_id is not null then
      canonical_id := (regexp_match(canonical_id, '[?&]id=([^&]+)'))[1];
      canonical_id := nullif(regexp_replace(canonical_id, '[[:cntrl:]]', '', 'g'), '');
    end if;
  end if;

  if canonical_id is null then
    return;
  end if;

  select
    array_agg(book_id order by has_high_res desc, cover_area desc, published_date desc nulls last, lower(title)) as ordered_ids,
    count(*) as total_books
  into book_ids, book_count
  from (
    select distinct
      b.id as book_id,
      b.title,
      b.published_date,
      coalesce(
        (
          select coalesce(bil.is_high_resolution, false)
          from book_image_links bil
          where bil.book_id = b.id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                   bil.created_at desc
          limit 1
        ),
        false
      ) as has_high_res,
      coalesce(
        (
          select coalesce((bil.width::bigint * bil.height::bigint), 0)
          from book_image_links bil
          where bil.book_id = b.id
          order by coalesce(bil.is_high_resolution, false) desc,
                   coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                   bil.created_at desc
          limit 1
        ),
        0
      ) as cover_area
    from book_external_ids bei
    join books b on b.id = bei.book_id
    where bei.source = 'GOOGLE_BOOKS'
      and (
        nullif(regexp_replace(bei.google_canonical_id, '[[:cntrl:]]', '', 'g'), '') = canonical_id
        or (
          bei.canonical_volume_link is not null
          and (regexp_match(bei.canonical_volume_link, '[?&]id=([^&]+)'))[1] = canonical_id
        )
      )
  ) ranked;

  if book_count is null or book_count < 2 then
    return;
  end if;

  -- Get primary title with fallback for NULL/empty
  select coalesce(nullif(title, ''), 'Untitled Book') into primary_title
  from books
  where id = book_ids[1];

  -- Ensure primary_title is never NULL
  if primary_title is null or primary_title = '' then
    primary_title := 'Untitled Book';
  end if;

  select id into cluster_uuid
  from work_clusters
  where google_canonical_id = canonical_id;

  if cluster_uuid is null then
    insert into work_clusters (google_canonical_id, canonical_title, confidence_score, cluster_method, member_count)
    values (canonical_id, primary_title, 0.85, 'GOOGLE_CANONICAL', book_count)
    returning id into cluster_uuid;
  else
    update work_clusters
    set canonical_title = coalesce(primary_title, work_clusters.canonical_title),
        member_count = book_count,
        updated_at = now()
    where id = cluster_uuid;
  end if;

  for i in 1..array_length(book_ids, 1) loop
    current_book := book_ids[i];

    insert into work_cluster_members (cluster_id, book_id, is_primary, confidence, join_reason)
    values (cluster_uuid, current_book, (i = 1), 0.85, 'GOOGLE_CANONICAL')
    on conflict (cluster_id, book_id) do update set
      is_primary = excluded.is_primary,
      confidence = excluded.confidence,
      join_reason = excluded.join_reason,
      joined_at = now();
  end loop;
end;
$$ language plpgsql;

create or replace function notify_primary_edition_change()
returns trigger as $$
begin
  if OLD.is_primary is true
     and (NEW.is_primary is distinct from OLD.is_primary)
     and (NEW.is_primary is null or NEW.is_primary = false) then
    insert into events_outbox (topic, payload, created_at)
    values (
      '/topic/cluster.' || OLD.cluster_id,
      jsonb_build_object(
        'event', 'primary_changed',
        'old_primary_book_id', OLD.book_id::text,
        'cluster_id', OLD.cluster_id::text,
        'timestamp', floor(extract(epoch from now()) * 1000)
      ),
      now()
    );
  end if;

  if NEW.is_primary is true
     and (NEW.is_primary is distinct from OLD.is_primary) then
    insert into events_outbox (topic, payload, created_at)
    values (
      '/topic/cluster.' || NEW.cluster_id,
      jsonb_build_object(
        'event', 'primary_changed',
        'new_primary_book_id', NEW.book_id::text,
        'cluster_id', NEW.cluster_id::text,
        'timestamp', floor(extract(epoch from now()) * 1000)
      ),
      now()
    );
  end if;

  return NEW;
end;
$$ language plpgsql;

drop trigger if exists work_cluster_primary_change on work_cluster_members;
create trigger work_cluster_primary_change
  after update on work_cluster_members
  for each row
  when (OLD.is_primary is distinct from NEW.is_primary)
  execute function notify_primary_edition_change();

-- Get all editions of a book
create or replace function get_book_editions(target_book_id uuid)
returns table (
  book_id uuid,
  title text,
  subtitle text,
  isbn13 text,
  isbn10 text,
  publisher text,
  published_date date,
  is_primary boolean,
  confidence float,
  cluster_method text
) as $$
begin
  return query
  select distinct
    b.id,
    b.title,
    b.subtitle,
    b.isbn13,
    b.isbn10,
    b.publisher,
    b.published_date,
    wcm.is_primary,
    wcm.confidence,
    wc.cluster_method
  from work_cluster_members wcm1
  join work_cluster_members wcm on wcm.cluster_id = wcm1.cluster_id
  join books b on b.id = wcm.book_id
  join work_clusters wc on wc.id = wcm.cluster_id
  where wcm1.book_id = target_book_id
    and wcm.book_id != target_book_id
  order by wcm.is_primary desc, wcm.confidence desc, b.published_date desc;
end;
$$ language plpgsql;

-- Cluster books by Google canonical links
create or replace function cluster_books_by_google_canonical()
returns table (clusters_created integer, books_clustered integer) as $$
declare
  rec record;
  cluster_uuid uuid;
  clusters_count integer := 0;
  books_count integer := 0;
  google_id text;
  existing_cluster uuid;
  primary_title text;
begin
  for rec in
    select
      canonical_id,
      array_agg(book_id order by has_high_res desc, cover_area desc, published_date desc nulls last, lower(title)) as book_ids,
      count(*) as book_count
    from (
      select distinct
        coalesce(
          nullif(regexp_replace(bei.google_canonical_id, '[[:cntrl:]]', '', 'g'), ''),
          (regexp_match(bei.canonical_volume_link, '[?&]id=([^&]+)'))[1]
        ) as canonical_id,
        b.id as book_id,
        b.title,
        b.published_date,
        coalesce(
          (
            select coalesce(bil.is_high_resolution, false)
            from book_image_links bil
            where bil.book_id = b.id
            order by coalesce(bil.is_high_resolution, false) desc,
                     coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                     bil.created_at desc
            limit 1
          ),
          false
        ) as has_high_res,
        coalesce(
          (
            select coalesce((bil.width::bigint * bil.height::bigint), 0)
            from book_image_links bil
            where bil.book_id = b.id
            order by coalesce(bil.is_high_resolution, false) desc,
                     coalesce((bil.width::bigint * bil.height::bigint), 0) desc,
                     bil.created_at desc
            limit 1
          ),
          0
        ) as cover_area
      from book_external_ids bei
      join books b on b.id = bei.book_id
      where bei.source = 'GOOGLE_BOOKS'
        and (
          bei.google_canonical_id is not null
          or bei.canonical_volume_link ~ '[?&]id='
        )
    ) candidate
    where canonical_id is not null
      and canonical_id <> ''
      and canonical_id !~ '[[:cntrl:]]'
    group by canonical_id
    having count(*) > 1
  loop
    if rec.book_ids is null or array_length(rec.book_ids, 1) = 0 then
      continue;
    end if;

    -- Get the primary title from the first (best) book with fallback for NULL/empty
    select coalesce(nullif(title, ''), 'Untitled Book') into primary_title
    from books
    where id = rec.book_ids[1];

    -- Ensure primary_title is never NULL
    if primary_title is null or primary_title = '' then
      primary_title := 'Untitled Book';
    end if;

    select id into cluster_uuid
    from work_clusters
    where google_canonical_id = rec.canonical_id;

    if cluster_uuid is null then
      insert into work_clusters (google_canonical_id, canonical_title, confidence_score, cluster_method, member_count)
      values (rec.canonical_id, primary_title, 0.85, 'GOOGLE_CANONICAL', rec.book_count)
      returning id into cluster_uuid;

      clusters_count := clusters_count + 1;
    else
      update work_clusters
      set canonical_title = coalesce(primary_title, work_clusters.canonical_title),
          member_count = rec.book_count,
          updated_at = now()
      where id = cluster_uuid;
    end if;

    for i in 1..array_length(rec.book_ids, 1) loop
      insert into work_cluster_members (cluster_id, book_id, is_primary, confidence, join_reason)
      values (cluster_uuid, rec.book_ids[i], (i = 1), 0.85, 'GOOGLE_CANONICAL')
      on conflict (cluster_id, book_id) do update set
        is_primary = excluded.is_primary,
        confidence = excluded.confidence,
        join_reason = excluded.join_reason,
        joined_at = now();

      books_count := books_count + 1;
    end loop;
  end loop;

  return query select clusters_count, books_count;
end;
$$ language plpgsql;

-- Statistics function
create or replace function get_clustering_stats()
returns table (
  total_books bigint,
  clustered_books bigint,
  unclustered_books bigint,
  total_clusters bigint,
  isbn_clusters bigint,
  google_clusters bigint,
  avg_cluster_size numeric
) as $$
begin
  return query
  with stats as (
    select
      (select count(*) from books) as total_books,
      (select count(distinct book_id) from work_cluster_members) as clustered_books,
      (select count(*) from work_clusters) as total_clusters,
      (select count(*) from work_clusters where cluster_method = 'ISBN_PREFIX') as isbn_clusters,
      (select count(*) from work_clusters where cluster_method = 'GOOGLE_CANONICAL') as google_clusters
  )
  select
    s.total_books,
    s.clustered_books,
    s.total_books - s.clustered_books as unclustered_books,
    s.total_clusters,
    s.isbn_clusters,
    s.google_clusters,
    case
      when s.total_clusters > 0
      then round(s.clustered_books::numeric / s.total_clusters, 2)
      else 0
    end as avg_cluster_size
  from stats s;
end;
$$ language plpgsql;

comment on function extract_isbn_work_prefix is 'Extracts work identifier from ISBN-13 (first 11 digits)';
comment on function cluster_books_by_isbn is 'Groups books by ISBN prefix to find editions of the same work';
comment on function cluster_books_by_google_canonical is 'Groups books by Google canonical volume link to find editions';
comment on function get_book_editions is 'Returns all editions of a book from its work cluster';
comment on function get_clustering_stats is 'Returns statistics about work clustering';

-- ============================================================================
-- TITLE + AUTHOR CLUSTERING
-- Merge duplicate clusters for books with same title/authors but different ISBNs
-- ============================================================================

-- Function to normalize titles for matching (remove punctuation, lowercase, trim)
create or replace function normalize_title_for_clustering(title text)
returns text as $$
begin
  if title is null then
    return null;
  end if;
  
  -- Lowercase, remove punctuation, collapse whitespace
  return trim(regexp_replace(
    lower(regexp_replace(title, '[^\w\s]', '', 'g')),
    '\s+', ' ', 'g'
  ));
end;
$$ language plpgsql immutable;

-- Function to get normalized author list for a book
create or replace function get_normalized_authors(book_id_param uuid)
returns text as $$
begin
  return (
    select string_agg(
      trim(lower(regexp_replace(regexp_replace(a.name, '[^\w\s]', '', 'g'), '\s+', ' ', 'g'))),
      '|'
      order by ba.position, a.name
    )
    from book_authors_join ba
    join authors a on a.id = ba.author_id
    where ba.book_id = book_id_param
  );
end;
$$ language plpgsql stable;

comment on function normalize_title_for_clustering is 'Normalize book title for clustering: lowercase, remove punctuation, collapse whitespace';
comment on function get_normalized_authors is 'Get normalized author list for a book as pipe-separated string';

-- ============================================================================
-- BOOK AI CONTENT
-- Stores versioned AI-generated analysis content per book.
-- ============================================================================

create table if not exists book_ai_content (
  id text primary key, -- NanoID (12 chars via IdGenerator.generateLong())
  book_id uuid not null references books(id) on delete cascade,
  version_number integer not null check (version_number > 0),
  is_current boolean not null default false,
  analysis_json jsonb not null,
  summary text not null,
  reader_fit text not null,
  key_themes jsonb not null,
  takeaways jsonb,
  context text,
  model text not null,
  provider text not null,
  prompt_hash text,
  created_at timestamptz not null default now(),
  unique (book_id, version_number)
);

create unique index if not exists uq_book_ai_content_current
  on book_ai_content(book_id)
  where is_current;

create index if not exists idx_book_ai_content_book_created
  on book_ai_content(book_id, created_at desc);

comment on table book_ai_content is 'Versioned AI-generated book analysis content with single current version per book';
comment on column book_ai_content.analysis_json is 'Canonical JSON payload returned by the AI model';
comment on column book_ai_content.reader_fit is 'Who should read this book and why';
comment on column book_ai_content.key_themes is 'JSON array of short key-theme strings';
comment on column book_ai_content.takeaways is 'JSON array of specific insight/point strings';
comment on column book_ai_content.context is 'Genre or field placement note';
